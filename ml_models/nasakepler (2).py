# -*- coding: utf-8 -*-
"""NASAkepler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l1u9vAtV1lfhXuiv5CrZBkr4dGWh-iW4
"""

from sklearn.model_selection import GroupKFold
import joblib, json, os
import warnings
warnings.filterwarnings("ignore")
from sklearn.metrics import roc_auc_score, average_precision_score
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import shap

path = "/content/drive/MyDrive/Makine_öğrenmesi/NASA/işlenmiş kepler.csv"

df = pd.read_csv(path)

display(df.head())

df.shape

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
print("Her sütundaki eksik değerlerin toplamı:")
display(df.isnull().sum())

median_koi_insol = df['koi_insol'].median()
df['koi_insol'] = df['koi_insol'].fillna(median_koi_insol)
df['koi_depth'] = df['koi_depth'].fillna(median_koi_insol)

print(df['koi_depth'].isnull().sum())
print(df['koi_insol'].isnull().sum())

df.info()

df.describe()

df.columns

# Label mapping: CONFIRMED/CANDIDATE → 1, FALSE POSITIVE → 0
label_map = {"CONFIRMED": 1, "CANDIDATE": 1, "FALSE POSITIVE": 0}
df["label"] = (
    df["koi_disposition"]
    .astype(str).str.upper().str.strip()
    .map(label_map)
)

df = df.dropna(subset=["label"]).copy()
df["label"] = df["label"].astype(int)

print("Label dağılımı:")
print(df["label"].value_counts())

feature_cols = [
    "koi_period", "koi_duration", "koi_depth", "koi_model_snr",
    "koi_num_transits", "koi_score", "koi_teq", "koi_prad", "koi_sma",
    "koi_steff", "koi_srad", "koi_smass",
    "koi_fpflag_nt", "koi_fpflag_ss", "koi_fpflag_co", "koi_fpflag_ec"
]

feature_cols = [c for c in feature_cols if c in df.columns]
print(f"Kullanılacak özellik sayısı: {len(feature_cols)}")
print(feature_cols)

X = df[feature_cols].copy()
y = df["label"].copy()

flag_cols = [c for c in ["koi_fpflag_nt", "koi_fpflag_ss", "koi_fpflag_co", "koi_fpflag_ec"] if c in X.columns]

for c in X.columns:
    X[c] = pd.to_numeric(X[c], errors="coerce")

for c in flag_cols:
    X[c] = X[c].fillna(0).astype("int64")

X = X.fillna(X.median(numeric_only=True))

nunique = X.nunique(dropna=False)
keep_cols = nunique[nunique > 1].index
X = X[keep_cols].copy()

print("X shape:", X.shape, "| y shape:", y.shape)

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
aucs, aps, models = [], [], []

for fold, (tr, va) in enumerate(skf.split(X, y), 1):
    X_tr, X_va = X.iloc[tr], X.iloc[va]
    y_tr, y_va = y.iloc[tr], y.iloc[va]

    model = lgb.LGBMClassifier(
        n_estimators=10000,
        learning_rate=0.05,
        max_depth=-1,
        subsample=0.8,
        colsample_bytree=0.8,
        class_weight='balanced',
        random_state=42
    )

    model.fit(
        X_tr, y_tr,
        eval_set=[(X_va, y_va)],
        eval_metric="auc",
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]  # verbose kapalı
    )

    proba = model.predict_proba(X_va, num_iteration=model.best_iteration_)[:, 1]
    auc  = roc_auc_score(y_va, proba)
    ap   = average_precision_score(y_va, proba)

    aucs.append(auc); aps.append(ap); models.append(model)
    print(f"Fold {fold}: AUC={auc:.3f}, AP={ap:.3f}, best_iter={model.best_iteration_}")

print("\nMean AUC:", np.mean(aucs))
print("Mean AP :", np.mean(aps))

best_model = models[int(np.argmax(aucs))]

importances = best_model.feature_importances_
order = np.argsort(importances)[::-1][:20]
top20 = [(X.columns[i], int(importances[i])) for i in order]
for name, imp in top20:
    print(f"{name:25s} {imp}")

id_col = "kepid"
uniq_key = "kepoi_name"

df = df.drop_duplicates(subset=[uniq_key]).copy()

core_feats = [
    "koi_period","koi_duration","koi_depth","koi_model_snr",
    "koi_num_transits","koi_teq","koi_prad","koi_sma",
    "koi_steff","koi_srad","koi_smass"
]
core_feats = [c for c in core_feats if c in df.columns]

suspect_feats = [c for c in ["koi_score","koi_fpflag_nt","koi_fpflag_ss","koi_fpflag_co","koi_fpflag_ec"] if c in df.columns]

def eval_cv(feats, title):
    X = df[feats].apply(pd.to_numeric, errors="coerce")
    y = df["label"].astype(int)
    groups = df[id_col]

    gkf = GroupKFold(n_splits=5)
    aucs, aps = [], []
    for tr, va in gkf.split(X, y, groups):
        X_tr, X_va = X.iloc[tr], X.iloc[va]
        y_tr, y_va = y.iloc[tr], y.iloc[va]

        imp = SimpleImputer(strategy="median")
        X_tr = imp.fit_transform(X_tr)
        X_va = imp.transform(X_va)

        model = lgb.LGBMClassifier(
            n_estimators=10000,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            class_weight="balanced",
            random_state=42
        )
        model.fit(
            X_tr, y_tr,
            eval_set=[(X_va, y_va)],
            eval_metric="auc",
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        proba = model.predict_proba(X_va, num_iteration=model.best_iteration_)[:,1]
        aucs.append(roc_auc_score(y_va, proba))
        aps.append(average_precision_score(y_va, proba))

    print(f"{title} -> AUC={np.mean(aucs):.4f}, AP={np.mean(aps):.4f}")

# ---- 1) Çekirdek (güvenli) özelliklerle CV
eval_cv(core_feats, "CORE only (transit + star phys)")

# ---- 2) Çekirdeğe şüphelileri ekleyip CV
if suspect_feats:
    eval_cv(core_feats + suspect_feats, "CORE + suspect (score/flags)")

# ---- 3) Sadece suspect ile CV (etkiyi görmek için)
if suspect_feats:
    eval_cv(suspect_feats, "SUSPECT only")

for c in core_feats + suspect_feats:
    s = pd.to_numeric(df[c], errors="coerce")
    corr = np.corrcoef(s.fillna(s.median()), df["label"])[0,1]
    print(f"{c:15s} corr≈ {corr:.3f}")

df.drop(columns=['koi_score'], inplace=True)
df.drop(columns=['koi_fpflag_nt'], inplace=True)
df.drop(columns=['koi_fpflag_ss'], inplace=True)
df.drop(columns=['koi_fpflag_co'], inplace=True)

if "kepoi_name" in df.columns:
    df = df.drop_duplicates(subset=["kepoi_name"]).copy()

core_feats = [
    "koi_period","koi_duration","koi_depth","koi_model_snr",
    "koi_num_transits","koi_teq","koi_prad","koi_sma",
    "koi_steff","koi_srad","koi_smass"
]
core_feats = [c for c in core_feats if c in df.columns]

X = df[core_feats].apply(pd.to_numeric, errors="coerce")
y = df["label"].astype(int)

groups = df["kepid"] if "kepid" in df.columns else df["kepoi_name"]


gkf = GroupKFold(n_splits=5)
aucs, aps, models = [], [], []

for fold, (tr, va) in enumerate(gkf.split(X, y, groups), 1):
    X_tr, X_va = X.iloc[tr], X.iloc[va]
    y_tr, y_va = y.iloc[tr], y.iloc[va]


    imp = SimpleImputer(strategy="median")
    X_tr = imp.fit_transform(X_tr)
    X_va = imp.transform(X_va)

    model = lgb.LGBMClassifier(
        n_estimators=10000,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        max_depth=-1,
        class_weight="balanced",
        random_state=42,
    )
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_va, y_va)],
        eval_metric="auc",
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)],
    )

    proba = model.predict_proba(X_va, num_iteration=model.best_iteration_)[:, 1]
    aucs.append(roc_auc_score(y_va, proba))
    aps.append(average_precision_score(y_va, proba))
    models.append((model, imp))
    print(f"Fold {fold}: AUC={aucs[-1]:.4f}, AP={aps[-1]:.4f}, best_iter={model.best_iteration_}")

print("\nMean AUC:", float(np.mean(aucs)))
print("Mean AP :", float(np.mean(aps)))


imp_final = SimpleImputer(strategy="median")
X_all = imp_final.fit_transform(X)

best_idx = int(np.argmax(aucs))
best_iter = models[best_idx][0].best_iteration_ or 1000

final_model = lgb.LGBMClassifier(
    n_estimators=best_iter,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    max_depth=-1,
    class_weight="balanced",
    random_state=42,
)
final_model.fit(X_all, y)

os.makedirs("models", exist_ok=True)
joblib.dump(final_model, "models/kepler_core_lgbm.pkl")
joblib.dump(imp_final,   "models/kepler_core_imputer.pkl")
with open("models/kepler_core_features.json", "w") as f:
    json.dump({"feature_cols": core_feats}, f, indent=2)

print("Kaydedildi: models/kepler_core_lgbm.pkl + kepler_core_imputer.pkl + kepler_core_features.json")

importances = final_model.feature_importances_
order = np.argsort(importances)[::-1][:20]
for i in order:
    print(f"{core_feats[i]:20s} {int(importances[i])}")

def add_engineered(X: pd.DataFrame) -> pd.DataFrame:
    X = X.copy()
    eps = 1e-6
    X["dur_per_ratio"]   = X["koi_duration"] / (X["koi_period"] + eps)
    X["depth_dur_ratio"] = X["koi_depth"] / (X["koi_duration"] + eps)
    X["prad_srad_ratio"] = X["koi_prad"] / (X["koi_srad"] + eps)
    X["teq_teff_ratio"]  = X["koi_teq"] / (X["koi_steff"] + eps)
    X["sma_srad_ratio"]  = X["koi_sma"] / (X["koi_srad"] + eps)
    X["depth_x_snr"]     = X["koi_depth"] * X["koi_model_snr"]
    X["period_x_transits"] = X["koi_period"] * X["koi_num_transits"]
    X["flag_large_prad"]   = (X["koi_prad"] > 20).astype(int)
    X["flag_invalid_dur"]  = (X["koi_duration"] > X["koi_period"]).astype(int)
    return X


def eval_model(X, y, label):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    aucs, aps = [], []
    for tr, va in skf.split(X, y):
        imp = SimpleImputer(strategy="median")
        X_tr = imp.fit_transform(X.iloc[tr])
        X_va = imp.transform(X.iloc[va])

        model = lgb.LGBMClassifier(
            n_estimators=5000, learning_rate=0.05,
            class_weight="balanced", random_state=42
        )
        model.fit(X_tr, y.iloc[tr],
                  eval_set=[(X_va, y.iloc[va])],
                  eval_metric="auc",
                  callbacks=[lgb.early_stopping(50), lgb.log_evaluation(-1)])
        p = model.predict_proba(X_va, num_iteration=model.best_iteration_)[:,1]
        aucs.append(roc_auc_score(y.iloc[va], p))
        aps.append(average_precision_score(y.iloc[va], p))
    print(f"{label:<20s} | AUC={np.mean(aucs):.4f} | AP={np.mean(aps):.4f}")


core_cols = [
    "koi_period","koi_duration","koi_depth","koi_model_snr",
    "koi_num_transits","koi_teq","koi_prad","koi_sma",
    "koi_steff","koi_srad","koi_smass"
]
core_cols = [c for c in core_cols if c in X.columns]
X_core = X[core_cols]

# Core + engineered
X_core_plus = add_engineered(X_core)

# Sadece engineered (orijinal core çıkarılıyor)
X_eng = add_engineered(X_core)
X_eng = X_eng.drop(columns=core_cols, errors="ignore")

eval_model(X_core, y, "Core only")
eval_model(X_core_plus, y, "Core + engineered")
eval_model(X_eng, y, "Engineered only")

def mean_importance(X, y, n_splits=5):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    feats = X.columns.tolist()
    imps = np.zeros(len(feats))
    for tr, va in skf.split(X, y):
        imp = SimpleImputer(strategy="median")
        Xtr = imp.fit_transform(X.iloc[tr]); ytr = y.iloc[tr]
        Xva = imp.transform(X.iloc[va]);     yva = y.iloc[va]
        m = lgb.LGBMClassifier(n_estimators=3000, learning_rate=0.05,
                               class_weight="balanced", random_state=42)
        m.fit(Xtr, ytr, eval_set=[(Xva, yva)],
              eval_metric="auc",
              callbacks=[lgb.early_stopping(50), lgb.log_evaluation(-1)])
        imps += m.feature_importances_
    imps /= n_splits
    return pd.Series(imps, index=feats).sort_values(ascending=False)

# 1) önce mevcut "Core + engineered" ile importanceları ölçtüm
imp_series = mean_importance(X_core_plus, y)

# 2) engineered olanlardan düşük önemlileri eledim
engineered_cols = [c for c in X_core_plus.columns if c not in X_core.columns]
drop_eng = [c for c in engineered_cols if imp_series.get(c, 0) <= 0]

# 3) sadeleştirilmiş setle yeniden değerlendirdim
X_slim = X_core_plus.drop(columns=drop_eng, errors="ignore")
print("Atılan engineered kolon sayısı:", len(drop_eng))

print("Nihai kullanılan özellik sayısı:", len(X_slim.columns))
display("Nihai feature listesi:\n", list(X_slim.columns))

importances = final_model.feature_importances_

feat_names = X.columns
imp_series = pd.Series(importances, index=feat_names).sort_values(ascending=False)

plt.figure(figsize=(8,6))

sns.barplot(x=imp_series.values[:15], y=imp_series.index[:15], palette="viridis")
plt.title("Top 15 Feature Importances")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

y_pred = final_model.predict_proba(X)[:,1]
fpr, tpr, _ = roc_curve(y, y_pred)

plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y, y_pred):.3f}")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid(True)
plt.show()

y_pred = final_model.predict_proba(X)[:,1]
prec, rec, _ = precision_recall_curve(y, y_pred)
ap = average_precision_score(y, y_pred)

plt.figure(figsize=(6,6))
plt.plot(rec, prec, label=f"AP = {ap:.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid(True)
plt.show()

y_pred = final_model.predict_proba(X)[:,1]
y_class = (y_pred > 0.5).astype(int)
cm = confusion_matrix(y, y_class, labels=[0,1])

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["False Positive","Planet"])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix (threshold=0.5)")
plt.show()

explainer = shap.TreeExplainer(final_model)
shap_values = explainer.shap_values(X)

shap.summary_plot(shap_values, X, plot_type="bar")
shap.summary_plot(shap_values, X)